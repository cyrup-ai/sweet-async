//! Emitting task builder for Tokio implementation
//!
//! This module provides the implementation for creating and configuring stream-based tasks.

use std::collections::HashMap;
use std::marker::PhantomData;
use std::sync::Arc;
use std::time::{Duration, Instant};

use futures::StreamExt;
use tokio::runtime::Handle;
use tokio::sync::{Mutex, oneshot, Semaphore, mpsc};
use tokio::task::JoinHandle;
use tokio_util::sync::CancellationToken;
use uuid::Uuid;

use sweet_async_api::task::builder::{
    AsyncTaskBuilder, AsyncWork, ReceiverStrategy, SenderStrategy,
};
use sweet_async_api::task::emit::{EmittingTask, EmittingTaskBuilder as ApiEmittingTaskBuilder};
use sweet_async_api::task::{AsyncTask, AsyncTaskError, TaskId, TaskPriority};

use super::async_work_wrapper::BoxedAsyncWork;
use super::event::{TokioEventSender, create_event_channel};
use crate::task::builder::TokioAsyncTaskBuilder;

/// Type alias for boxed async work that produces a channel receiver
type BoxedChannelWork<T> = BoxedAsyncWork<tokio::sync::mpsc::Receiver<T>>;

/// Tokio implementation of the EmittingTaskBuilder
#[derive(Clone)]
pub struct TokioEmittingTaskBuilder<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + Sync + 'static,
    I: TaskId,
> {
    /// Base builder with common configuration
    base_builder: TokioAsyncTaskBuilder<T, I>,
    /// Tokio runtime handle
    runtime: Handle,
    /// Active tasks registry
    active_tasks: Arc<Mutex<Vec<JoinHandle<()>>>>,
    /// Task priority
    priority: TaskPriority,
    /// Type markers
    _marker: PhantomData<(C, EItem, EOverall)>,
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + Sync + 'static,
    I: TaskId,
> TokioEmittingTaskBuilder<T, C, EItem, EOverall, I>
{
    /// Create a new emitting task builder
    pub fn new(runtime: Handle, active_tasks: Arc<Mutex<Vec<JoinHandle<()>>>>) -> Self {
        Self {
            base_builder: TokioAsyncTaskBuilder::new(runtime.clone(), active_tasks.clone()),
            runtime,
            active_tasks,
            priority: TaskPriority::Normal,
            _marker: PhantomData,
        }
    }

    /// Set the task priority
    pub fn priority(self, priority: TaskPriority) -> Self {
        Self { priority, ..self }
    }

    /// Set the task name
    pub fn name(self, name: &str) -> Self {
        Self {
            base_builder: self.base_builder.name(name),
            ..self
        }
    }
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + Sync + 'static,
    I: TaskId,
> AsyncTaskBuilder for TokioEmittingTaskBuilder<T, C, EItem, EOverall, I>
{
    fn timeout(self, duration: Duration) -> Self {
        Self {
            base_builder: self.base_builder.timeout(duration),
            ..self
        }
    }

    fn retry(self, attempts: u8) -> Self {
        Self {
            base_builder: self.base_builder.retry(attempts),
            ..self
        }
    }

    fn tracing(self, enabled: bool) -> Self {
        Self {
            base_builder: self.base_builder.tracing(enabled),
            ..self
        }
    }

    fn new() -> Self {
        let runtime = Handle::current();
        let active_tasks = Arc::new(Mutex::new(Vec::new()));
        Self::new(runtime, active_tasks)
    }
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + Sync + 'static,
    I: TaskId,
> ApiEmittingTaskBuilder<T, C, EItem, I> for TokioEmittingTaskBuilder<T, C, EItem, EOverall, I>
{
    type SenderBuilder = TokioSenderBuilder<T, C, EItem, EOverall, I>;

    fn sender<F>(
        self,
        sender_logic: F,
        strategy: SenderStrategy,
    ) -> Self::SenderBuilder
    where
        F: AsyncWork<T> + Send + 'static,
    {
        // The sender logic should produce a channel of T values
        // We need to adapt it to produce a Receiver<T>
        let channel_work = move || async move {
            let (tx, rx) = mpsc::channel(100);
            // Send the single value produced by the sender logic
            let value = sender_logic.run().await;
            let _ = tx.send(value).await;
            rx
        };
        
        TokioSenderBuilder {
            base_builder: self.base_builder,
            runtime: self.runtime,
            active_tasks: self.active_tasks,
            priority: self.priority,
            sender_work: BoxedAsyncWork::new(channel_work),
            sender_strategy: strategy,
            _marker: PhantomData,
        }
    }
}

impl<T: Clone + Send + Sync + 'static, Task: AsyncTask<T, I>, I: TaskId, E>
    sweet_async_api::orchestra::OrchestratorBuilder<T, Task, I>
    for TokioEmittingTaskBuilder<T, T, E, E, I>
where
    E: Send + Sync + 'static,
{
    type Next = TokioAsyncTaskBuilder<T, I>;

    fn orchestrator<O: sweet_async_api::orchestra::orchestrator::TaskOrchestrator<T, Task, I>>(
        self,
        _orchestrator: &O,
    ) -> Self::Next {
        self.base_builder
    }
}

/// Tokio implementation of the SenderBuilder
pub struct TokioSenderBuilder<T, C, EItem, EOverall, I>
where
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + 'static,
    I: TaskId,
{
    /// Base builder with common configuration
    base_builder: TokioAsyncTaskBuilder<T, I>,
    /// Tokio runtime handle
    runtime: Handle,
    /// Active tasks registry
    active_tasks: Arc<Mutex<Vec<JoinHandle<()>>>>,
    /// Task priority
    priority: TaskPriority,
    /// Event sender work function that produces a channel
    sender_work: BoxedChannelWork<T>,
    /// Sender strategy
    sender_strategy: SenderStrategy,
    /// Type markers
    _marker: PhantomData<(C, EItem, EOverall)>,
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + 'static,
    I: TaskId,
> sweet_async_api::task::emit::builder::SenderBuilder<T, C, EItem, I> 
    for TokioSenderBuilder<T, C, EItem, EOverall, I>
{
    type ReceiverBuilder = TokioReceiverBuilder<T, C, EItem, I>;

    fn receiver<F>(
        self,
        receiver_work: F,
        strategy: ReceiverStrategy,
    ) -> Self::ReceiverBuilder
    where
        F: AsyncWork<C> + Send + 'static,
    {
        TokioReceiverBuilder {
            base_builder: self.base_builder,
            runtime: self.runtime,
            active_tasks: self.active_tasks,
            priority: self.priority,
            sender_work: self.sender_work,
            sender_strategy: self.sender_strategy,
            receiver_work: BoxedAsyncWork::new(receiver_work),
            receiver_strategy: strategy,
            _marker: PhantomData,
        }
    }
}

/// Tokio implementation of ReceiverBuilder
pub struct TokioReceiverBuilder<T, C, EItem, I>
where 
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    I: TaskId,
{
    /// Base builder with common configuration
    base_builder: TokioAsyncTaskBuilder<T, I>,
    /// Tokio runtime handle
    runtime: Handle,
    /// Active tasks registry
    active_tasks: Arc<Mutex<Vec<JoinHandle<()>>>>,
    /// Task priority
    priority: TaskPriority,
    /// Event sender work function
    sender_work: BoxedChannelWork<T>,
    /// Sender strategy
    sender_strategy: SenderStrategy,
    /// Event receiver work
    receiver_work: BoxedAsyncWork<C>,
    /// Receiver strategy
    receiver_strategy: ReceiverStrategy,
    /// Type markers
    _marker: PhantomData<EItem>,
}

/// Tokio implementation of EmittingTask
pub struct TokioEmittingTask<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Send + Sync + 'static,
    EOverall: Send + Sync + 'static,
    I: TaskId,
> {
    /// Task ID
    id: I,
    /// Task priority
    priority: TaskPriority,
    /// Sender task handle
    sender_handle: Arc<Mutex<Option<JoinHandle<Result<(), AsyncTaskError>>>>>,
    /// Receiver task handle
    receiver_handle:
        Arc<Mutex<Option<JoinHandle<Result<HashMap<Uuid, Result<C, EItem>>, AsyncTaskError>>>>>,
    /// Event sender
    event_sender: Arc<TokioEventSender<T>>,
    /// Cancellation sender
    cancel_tx: Arc<Mutex<Option<oneshot::Sender<()>>>>,
    /// Final result from task completion
    final_result: Arc<Mutex<Option<Result<HashMap<Uuid, Result<C, EItem>>, AsyncTaskError>>>>,
    /// Task metrics
    metrics: crate::task::async_task::TaskMetrics,
    /// Task timeout
    task_timeout: Duration,
    /// Type markers
    _marker: PhantomData<EOverall>,
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Clone + Send + Sync + 'static,
    I: TaskId + Copy + Clone + Send + Sync + 'static,
> TokioEmittingTask<T, C, EItem, AsyncTaskError, I>
{
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        id: I,
        priority: TaskPriority,
        sender_work_produces_receiver: BoxedChannelWork<T>,
        sender_strategy: SenderStrategy,
        receiver_work: BoxedAsyncWork<C>,
        receiver_strategy: ReceiverStrategy,
        runtime: Handle,
        task_timeout_duration: Duration,
        active_tasks: Arc<Mutex<Vec<JoinHandle<()>>>>,
    ) -> Self {
        let (internal_event_tx, internal_event_rx_for_collector) = create_event_channel::<T>(100);
        let (cancel_tx_main_oneshot, mut cancel_rx_for_sender) = oneshot::channel();
        let (_receiver_task_cancel_tx, receiver_task_cancel_rx) = oneshot::channel();

        let sender_join_handle_arc = Arc::new(Mutex::new(None));
        let receiver_join_handle_arc = Arc::new(Mutex::new(None));
        let final_result_arc = Arc::new(Mutex::new(None));

        let runtime_for_sender = runtime.clone();
        let sender_join_handle_for_storage = sender_join_handle_arc.clone();
        let internal_event_tx_for_sender_task = internal_event_tx.clone();

        // Spawn the sender task
        let original_sender_jh: JoinHandle<Result<(), AsyncTaskError>> = runtime_for_sender.spawn(async move {
            let mut user_event_source_rx: tokio::sync::mpsc::Receiver<T> = sender_work_produces_receiver.run().await;
            let mut overall_sender_result: Result<(), AsyncTaskError> = Ok(());

            match sender_strategy {
                SenderStrategy::Serial { timeout_seconds } => {
                    let item_send_timeout = if timeout_seconds > 0 { Some(Duration::from_secs(timeout_seconds)) } else { None };
                    loop {
                        tokio::select! {
                            biased;
                            _ = &mut cancel_rx_for_sender => { return Err(AsyncTaskError::Cancelled); }
                            maybe_event_t = user_event_source_rx.recv() => {
                                if let Some(event_t) = maybe_event_t {
                                    let send_op = internal_event_tx_for_sender_task.send(event_t);
                                    let send_result = match item_send_timeout {
                                        Some(dur) => tokio::time::timeout(dur, send_op).await.map_err(|_| AsyncTaskError::Timeout(dur)),
                                        None => Ok(send_op.await),
                                    };
                                    if let Err(send_error) = send_result.and_then(|res| res.map_err(|_| AsyncTaskError::Failure("Internal channel send failed".to_string()))) {
                                        tracing::error!("SerialSender: Failed to send event to internal channel: {:?}", send_error);
                                        return Err(send_error);
                                    }
                                } else {
                                    break;
                                }
                            }
                        }
                    }
                }
                SenderStrategy::Parallel { workers, rate_limit: _ } => {
                    let semaphore = Arc::new(Semaphore::new(workers.min().max(1)));
                    let mut sender_join_handles: Vec<JoinHandle<Result<(), AsyncTaskError>>> = Vec::new();
                    let mut processing_error: Option<AsyncTaskError> = None;

                    loop {
                        if processing_error.is_some() { break; }
                        tokio::select! {
                            biased;
                            _ = &mut cancel_rx_for_sender => {
                                processing_error = Some(AsyncTaskError::Cancelled);
                                break;
                            }
                            maybe_event_t = user_event_source_rx.recv() => {
                                if let Some(event_t) = maybe_event_t {
                                    let permit = match semaphore.clone().acquire_owned().await { Ok(p) => p, Err(_) => { processing_error = Some(AsyncTaskError::Failure("Semaphore closed prematurely".into())); break; } };
                                    let tx_clone = internal_event_tx_for_sender_task.clone();
                                    let jh = tokio::spawn(async move {
                                        let send_result = tx_clone.send(event_t).await;
                                        drop(permit);
                                        if send_result.is_err() {
                                            tracing::warn!("ParallelSender: Failed to send event to internal channel.");
                                            return Err(AsyncTaskError::Failure("Internal channel send failed in parallel worker".to_string()));
                                        }
                                        Ok(())
                                    });
                                    sender_join_handles.push(jh);
                                } else {
                                    break;
                                }
                            }
                        }
                    }

                    if let Some(AsyncTaskError::Cancelled) = processing_error {
                        tracing::debug!("ParallelSender: Cancellation received. Aborting active send tasks.");
                        for handle in &sender_join_handles { handle.abort(); }
                    }

                    for handle in sender_join_handles {
                        match handle.await {
                            Ok(Ok(())) => { }
                            Ok(Err(e)) => {
                                tracing::error!("ParallelSender: A send task failed: {:?}", e);
                                if processing_error.is_none() { processing_error = Some(e); }
                            }
                            Err(join_error) => {
                                tracing::error!("ParallelSender: A send task panicked or was aborted: {:?}", join_error);
                                if processing_error.is_none() {
                                     if join_error.is_cancelled() {
                                        if !matches!(processing_error, Some(AsyncTaskError::Cancelled)) {
                                            processing_error = Some(AsyncTaskError::Cancelled);
                                        }
                                     } else {
                                        processing_error = Some(AsyncTaskError::Failure(format!("Send task panic: {}", join_error)));
                                     }
                                }
                            }
                        }
                    }
                    overall_sender_result = match processing_error {
                        Some(e) => Err(e),
                        None => Ok(()),
                    };
                }
                SenderStrategy::Batched { batch_size, max_delay } => {
                    let mut batch = Vec::with_capacity(batch_size);
                    let mut interval = tokio::time::interval(max_delay);
                    interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Delay);

                    loop {
                        tokio::select! {
                            biased;
                            _ = &mut cancel_rx_for_sender => { return Err(AsyncTaskError::Cancelled); }
                            _ = interval.tick() => {
                                if !batch.is_empty() {
                                    let current_batch = std::mem::take(&mut batch);
                                    tracing::debug!("BatchedSender: Sending batch of {} items due to timeout", current_batch.len());
                                    for event_t in current_batch {
                                        if internal_event_tx_for_sender_task.send(event_t).await.is_err() {
                                            tracing::error!("BatchedSender: Failed to send event to internal channel during batch send (timeout).");
                                            return Err(AsyncTaskError::Failure("Internal channel send failed in batched sender".to_string()));
                                        }
                                    }
                                }
                            }
                            maybe_event_t = user_event_source_rx.recv() => {
                                if let Some(event_t) = maybe_event_t {
                                    batch.push(event_t);
                                    if batch.len() >= batch_size {
                                        let current_batch = std::mem::take(&mut batch);
                                        tracing::debug!("BatchedSender: Sending batch of {} items due to size", current_batch.len());
                                        for event_t_in_batch in current_batch {
                                            if internal_event_tx_for_sender_task.send(event_t_in_batch).await.is_err() {
                                                tracing::error!("BatchedSender: Failed to send event to internal channel during batch send (size).");
                                                return Err(AsyncTaskError::Failure("Internal channel send failed in batched sender".to_string()));
                                            }
                                        }
                                        interval.reset();
                                    }
                                } else {
                                    if !batch.is_empty() {
                                        tracing::debug!("BatchedSender: Sending final batch of {} items from stream end", batch.len());
                                        for event_t_in_final_batch in batch {
                                            if internal_event_tx_for_sender_task.send(event_t_in_final_batch).await.is_err() {
                                                tracing::error!("BatchedSender: Failed to send event to internal channel during final batch send.");
                                                return Err(AsyncTaskError::Failure("Internal channel send failed in batched sender".to_string()));
                                            }
                                        }
                                    }
                                    break;
                                }
                            }
                        }
                    }
                }
                SenderStrategy::Adaptive { initial_capacity, max_concurrency, adaptation_window, use_rayon_for_cpu: _ } => {
                    tracing::debug!(
                        "AdaptiveSender: Init with initial_cap={}, max_con={}, window={:?}",
                        initial_capacity, max_concurrency, adaptation_window
                    );

                    let max_c = max_concurrency.max(1);
                    let min_c = 1;
                    let mut target_concurrency = initial_capacity.max(min_c).min(max_c);

                    let semaphore = Arc::new(Semaphore::new(max_c));

                    let mut active_send_subtasks: usize = 0;
                    let mut send_durations_micros: Vec<u128> = Vec::with_capacity(initial_capacity * 2);
                    let mut successful_sends_window: usize = 0;
                    let mut failed_sends_window: usize = 0;
                    let mut last_adaptation_time = Instant::now();
                    let adaptation_sample_min_count = initial_capacity.max(5);

                    let mut sender_task_join_handles = Vec::new();
                    let (metric_tx, mut metric_rx) = mpsc::channel::<Result<Duration, AsyncTaskError>>(max_c * 2);
                    let mut processing_error: Option<AsyncTaskError> = None;
                    let mut user_stream_ended = false;

                    loop {
                        if processing_error.is_some() { break; }

                        tokio::select! {
                            biased;
                            _ = &mut cancel_rx_for_sender => {
                                processing_error = Some(AsyncTaskError::Cancelled);
                                break;
                            }
                            Some(metric_result) = metric_rx.recv() => {
                                active_send_subtasks = active_send_subtasks.saturating_sub(1);
                                match metric_result {
                                    Ok(duration) => {
                                        send_durations_micros.push(duration.as_micros());
                                        successful_sends_window += 1;
                                    }
                                    Err(e) => {
                                        failed_sends_window += 1;
                                        tracing::warn!("AdaptiveSender: Worker send error: {:?}", e);
                                    }
                                }
                            }
                            maybe_event_t = user_event_source_rx.recv(), if !user_stream_ended && active_send_subtasks < target_concurrency => {
                                if let Some(event_t) = maybe_event_t {
                                    let permit = match semaphore.clone().try_acquire_owned() {
                                        Ok(p) => p,
                                        Err(_) => {
                                            match semaphore.clone().acquire_owned().await {
                                                Ok(p_block) => p_block,
                                                Err(_) => { processing_error = Some(AsyncTaskError::Failure("Semaphore closed during blocking acquire".into())); break; }
                                            }
                                        }
                                    };
                                    active_send_subtasks += 1;

                                    let tx_clone = internal_event_tx_for_sender_task.clone();
                                    let metric_tx_clone = metric_tx.clone();
                                    let jh = tokio::spawn(async move {
                                        let send_start_time = Instant::now();
                                        let send_res = tx_clone.send(event_t).await;
                                        let send_duration = send_start_time.elapsed();
                                        drop(permit);
                                        let task_result = if send_res.is_err() {
                                            Err(AsyncTaskError::Failure("Internal channel send failed in adaptive worker".to_string()))
                                        } else {
                                            Ok(send_duration)
                                        };
                                        if metric_tx_clone.send(task_result).await.is_err() {
                                            tracing::warn!("AdaptiveSender: Failed to send metric.");
                                        }
                                    });
                                    sender_task_join_handles.push(jh);
                                } else {
                                    user_stream_ended = true;
                                }
                            }
                        }

                        // Adaptation Logic
                        if last_adaptation_time.elapsed() >= adaptation_window &&
                           (successful_sends_window + failed_sends_window) >= adaptation_sample_min_count &&
                           processing_error.is_none() {

                            let total_samples = successful_sends_window + failed_sends_window;
                            let error_rate = if total_samples > 0 { failed_sends_window as f64 / total_samples as f64 } else { 0.0 };
                            let avg_latency_micros = if !send_durations_micros.is_empty() {
                                send_durations_micros.iter().sum::<u128>() / send_durations_micros.len() as u128
                            } else { 0 };

                            let mut new_target_concurrency = target_concurrency;
                            const HIGH_ERROR_RATE_THRESHOLD: f64 = 0.1;
                            const HIGH_LATENCY_MICROS: u128 = 50_000;
                            const LOW_LATENCY_MICROS: u128 = 10_000;

                            if error_rate > HIGH_ERROR_RATE_THRESHOLD {
                                new_target_concurrency = (target_concurrency as f64 * 0.75).round() as usize;
                                tracing::warn!("AdaptiveSender: High error rate ({:.2}%), reducing concurrency.", error_rate * 100.0);
                            } else if avg_latency_micros > HIGH_LATENCY_MICROS {
                                new_target_concurrency = (target_concurrency as f64 * 0.85).round() as usize;
                            } else if avg_latency_micros < LOW_LATENCY_MICROS && error_rate == 0.0 {
                                new_target_concurrency += 1;
                            }
                            new_target_concurrency = new_target_concurrency.max(min_c).min(max_c);

                            if new_target_concurrency != target_concurrency {
                                tracing::debug!(
                                    "AdaptiveSender: Adjusting target C from {} to {} (avg_lat: {}us, err_rate: {:.2}%)",
                                    target_concurrency, new_target_concurrency, avg_latency_micros, error_rate * 100.0
                                );
                                target_concurrency = new_target_concurrency;
                            }

                            send_durations_micros.clear();
                            successful_sends_window = 0;
                            failed_sends_window = 0;
                            last_adaptation_time = Instant::now();
                        }

                        if user_stream_ended && active_send_subtasks == 0 && metric_rx.try_recv().is_err() {
                            break;
                        }
                    }
                    
                    drop(metric_tx);
                    while let Some(metric_result) = metric_rx.recv().await {
                         match metric_result {
                            Ok(duration) => { send_durations_micros.push(duration.as_micros()); successful_sends_window += 1; }
                            Err(_) => { failed_sends_window += 1; }
                        }
                    }
                    if processing_error.is_some() && !matches!(processing_error, Some(AsyncTaskError::Cancelled)) {
                         if !cancel_rx_for_sender.try_recv().is_ok() {
                            for handle in &sender_task_join_handles { handle.abort(); }
                         }
                    } else if matches!(processing_error, Some(AsyncTaskError::Cancelled)) {
                         for handle in &sender_task_join_handles { handle.abort(); }
                    }

                    for handle in sender_task_join_handles {
                        match handle.await {
                            Ok(_) => {}
                            Err(join_error) => {
                                if processing_error.is_none() {
                                    processing_error = Some(if join_error.is_cancelled() { AsyncTaskError::Cancelled } else { AsyncTaskError::Failure(format!("Send sub-task panic: {}", join_error)) });
                                }
                            }
                        }
                    }
                    overall_sender_result = match processing_error {
                        Some(e) => Err(e),
                        None => Ok(()),
                    };
                }
            }
            overall_sender_result
        });

        {
            let sender_handle_clone = original_sender_jh;
            
            tokio::spawn(async move {
                *sender_join_handle_for_storage.lock().await = Some(sender_handle_clone);
            });
        }

        let final_result_for_receiver_task = final_result_arc.clone();
        let runtime_for_receiver = runtime.clone();
        let receiver_join_handle_for_storage = receiver_join_handle_arc.clone();
        
        // Create receiver function that uses the receiver work
        let receiver_work_clone = Arc::new(Mutex::new(Some(receiver_work)));
        let receiver_fn = move |_t: &T, _: &mut (), _uuid: Uuid| -> Result<C, EItem> {
            // This is a simplified version - in reality we'd transform T to C
            // For now, just use the receiver work to produce C
            let work = receiver_work_clone.blocking_lock().take();
            if let Some(w) = work {
                let handle = Handle::current();
                match handle.block_on(w.run()) {
                    c => Ok(c),
                }
            } else {
                Err(todo!()) // Convert to proper error
            }
        };

        let original_receiver_jh: JoinHandle<Result<HashMap<Uuid, Result<C, EItem>>, AsyncTaskError>> = runtime_for_receiver.spawn(async move {
            let mut local_collector = crate::task::emit::collector::TokioEventCollector::<T, C, EItem, I>::new();
            let collector_token = CancellationToken::new();

            local_collector.start_processing(
                internal_event_rx_for_collector,
                receiver_strategy,
                Arc::new(receiver_fn),
                collector_token.clone()
            );

            tokio::select! {
                biased;
                _ = receiver_task_cancel_rx => {
                    collector_token.cancel();
                    let collected_map_on_cancel = local_collector.join().await;
                    *final_result_for_receiver_task.lock().await = Some(Ok(collected_map_on_cancel.clone()));
                    Err(AsyncTaskError::Cancelled)
                }
                collected_items_map_result = local_collector.join() => {
                    *final_result_for_receiver_task.lock().await = Some(Ok(collected_items_map_result.clone()));
                    Ok(collected_items_map_result)
                }
            }
        });

        {
            let receiver_handle_clone = original_receiver_jh;
            
            tokio::spawn(async move {
                *receiver_join_handle_for_storage.lock().await = Some(receiver_handle_clone);
            });
        }

        Self {
            id,
            priority,
            sender_handle: sender_join_handle_arc,
            receiver_handle: receiver_join_handle_arc,
            event_sender: internal_event_tx,
            cancel_tx: Arc::new(Mutex::new(Some(cancel_tx_main_oneshot))),
            final_result: final_result_arc,
            metrics: crate::task::async_task::TaskMetrics::new(),
            task_timeout: task_timeout_duration,
            _marker: PhantomData,
        }
    }

    /// Waits for the emitting task to complete and returns a TokioFinalEvent or an error.
    pub async fn await_final_event(
        self,
    ) -> Result<crate::task::emit::TokioFinalEvent<(), C, EItem, I>, AsyncTaskError> {
        let task_id_for_final_event = self.id;
        let overall_task_timeout = self.task_timeout;
        let final_result_arc_clone = self.final_result.clone();
        let receiver_handle_arc_clone = self.receiver_handle.clone();

        let processing_future = async {
            let receiver_jh_opt = {
                let mut handle_guard = receiver_handle_arc_clone.lock().await;
                handle_guard.take()
            };

            if let Some(receiver_jh) = receiver_jh_opt {
                match receiver_jh.await {
                    Ok(task_outcome_result) => match task_outcome_result {
                        Ok(collected_map_with_results) => {
                            let final_event = crate::task::emit::TokioFinalEvent::new(
                                (),
                                collected_map_with_results,
                                task_id_for_final_event,
                            );
                            Ok(final_event)
                        }
                        Err(async_task_error) => Err(async_task_error),
                    },
                    Err(join_error) => {
                        let error_msg = format!("Receiver task join error: {}", join_error);
                        {
                            let mut fr_guard = final_result_arc_clone.lock().await;
                            if fr_guard.is_none() {
                                *fr_guard = Some(Err(AsyncTaskError::Failure(error_msg.clone())));
                            }
                        }
                        Err(AsyncTaskError::Failure(error_msg))
                    }
                }
            } else {
                let final_result_outcome = {
                    let guard = final_result_arc_clone.lock().await;
                    guard.as_ref().map(|r| match r {
                        Ok(map) => Ok(map.clone()),
                        Err(e) => Err(AsyncTaskError::Failure(format!("{}", e))),
                    })
                };
                match final_result_outcome {
                    Some(Ok(collected_map_with_results)) => {
                        let final_event = crate::task::emit::TokioFinalEvent::new(
                            (),
                            collected_map_with_results,
                            task_id_for_final_event,
                        );
                        Ok(final_event)
                    }
                    Some(Err(async_task_error)) => Err(async_task_error),
                    None => Err(AsyncTaskError::Failure(
                        "Emitting task result unavailable and handle already taken/not set"
                            .to_string(),
                    )),
                }
            }
        };

        if overall_task_timeout > Duration::ZERO {
            match tokio::time::timeout(overall_task_timeout, processing_future).await {
                Ok(result_of_processing) => result_of_processing,
                Err(_) => {
                    {
                        let mut fr_guard = final_result_arc_clone.lock().await;
                        if fr_guard.is_none() || matches!(*fr_guard, Some(Ok(_))) {
                            *fr_guard = Some(Err(AsyncTaskError::Timeout(overall_task_timeout)));
                        }
                    }
                    Err(AsyncTaskError::Timeout(overall_task_timeout))
                }
            }
        } else {
            processing_future.await
        }
    }
}

impl<
    T: Clone + Send + Sync + 'static,
    C: Clone + Send + Sync + 'static,
    EItem: Clone + Send + Sync + 'static,
    I: TaskId + Copy + Clone + Send + Sync + 'static,
> TokioEmittingTask<T, C, EItem, AsyncTaskError, I>
{
    /// Waits for the emitting task to complete and returns a TokioFinalEvent or an error.
    pub async fn await_final_event(
        self,
    ) -> Result<crate::task::emit::TokioFinalEvent<(), C, EItem, I>, AsyncTaskError> {
        let task_id_for_final_event = self.id;
        let overall_task_timeout = self.task_timeout;
        let final_result_arc_clone = self.final_result.clone();
        let receiver_handle_arc_clone = self.receiver_handle.clone();

        let processing_future = async {
            let receiver_jh_opt = {
                let mut handle_guard = receiver_handle_arc_clone.lock().await;
                handle_guard.take()
            };

            if let Some(receiver_jh) = receiver_jh_opt {
                match receiver_jh.await {
                    Ok(task_outcome_result) => match task_outcome_result {
                        Ok(collected_map_with_results) => {
                            let final_event = crate::task::emit::TokioFinalEvent::new(
                                (),
                                collected_map_with_results,
                                task_id_for_final_event,
                            );
                            Ok(final_event)
                        }
                        Err(async_task_error) => Err(async_task_error),
                    },
                    Err(join_error) => {
                        let error_msg = format!("Receiver task join error: {}", join_error);
                        {
                            let mut fr_guard = final_result_arc_clone.lock().await;
                            if fr_guard.is_none() {
                                *fr_guard = Some(Err(AsyncTaskError::Failure(error_msg.clone())));
                            }
                        }
                        Err(AsyncTaskError::Failure(error_msg))
                    }
                }
            } else {
                let final_result_outcome = {
                    let guard = final_result_arc_clone.lock().await;
                    guard.as_ref().map(|r| match r {
                        Ok(map) => Ok(map.clone()),
                        Err(e) => Err(AsyncTaskError::Failure(format!("{}", e))),
                    })
                };
                match final_result_outcome {
                    Some(Ok(collected_map_with_results)) => {
                        let final_event = crate::task::emit::TokioFinalEvent::new(
                            (),
                            collected_map_with_results,
                            task_id_for_final_event,
                        );
                        Ok(final_event)
                    }
                    Some(Err(async_task_error)) => Err(async_task_error),
                    None => Err(AsyncTaskError::Failure(
                        "Emitting task result unavailable and handle already taken/not set"
                            .to_string(),
                    )),
                }
            }
        };

        if overall_task_timeout > Duration::ZERO {
            match tokio::time::timeout(overall_task_timeout, processing_future).await {
                Ok(result_of_processing) => result_of_processing,
                Err(_) => {
                    {
                        let mut fr_guard = final_result_arc_clone.lock().await;
                        if fr_guard.is_none() || matches!(*fr_guard, Some(Ok(_))) {
                            *fr_guard = Some(Err(AsyncTaskError::Timeout(overall_task_timeout)));
                        }
                    }
                    Err(AsyncTaskError::Timeout(overall_task_timeout))
                }
            }
        } else {
            processing_future.await
        }
    }
}

// Implement core AsyncTask methods first (these are required by EmittingTask trait)
impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId> AsyncTask<T, I>
    for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn to<R: Send + 'static, Task: AsyncTask<R, I>>()
    -> impl sweet_async_api::orchestra::OrchestratorBuilder<R, Task, I> {
        use crate::builder::DefaultOrchestratorBuilder;
        DefaultOrchestratorBuilder::<R, Task, I>::new_spawning()
    }

    fn emits<R: Send + 'static, Task: AsyncTask<R, I>>()
    -> impl sweet_async_api::orchestra::OrchestratorBuilder<R, Task, I> {
        use crate::builder::DefaultOrchestratorBuilder;
        DefaultOrchestratorBuilder::<R, Task, I>::new_emitting()
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId + AsRef<Uuid>>
    EmittingTask<T, C, EItem, I> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    type Final = crate::task::emit::TokioFinalEvent<(), C, EItem, I>;

    fn is_complete(&self) -> bool {
        match self.final_result.try_lock() {
            Ok(result) => result.is_some(),
            Err(_) => false,
        }
    }

    fn cancel(&self) -> Result<(), sweet_async_api::orchestra::OrchestratorError> {
        let cancel_tx = Arc::clone(&self.cancel_tx);
        
        match cancel_tx.try_lock() {
            Ok(mut guard) => {
                if let Some(sender) = guard.take() {
                    let _ = sender.send(());
                }
                Ok(())
            }
            Err(_) => {
                tokio::spawn(async move {
                    let mut guard = cancel_tx.lock().await;
                    if let Some(sender) = guard.take() {
                        let _ = sender.send(());
                    }
                });
                
                Ok(())
            }
        }
    }
}

// Implement all required AsyncTask supertraits
impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::PrioritizedTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn priority(&self) -> &impl sweet_async_api::task::RankableByPriority {
        &self.priority
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::CancellableTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    async fn cancel(
        &self,
        _level: sweet_async_api::task::CancellationLevel,
    ) -> Result<(), sweet_async_api::orchestra::OrchestratorError> {
        let cancel_tx = self.cancel_tx.clone();

        let sender = {
            let mut guard = cancel_tx.lock().await;
            guard.take()
        };

        if let Some(tx) = sender {
            let _ = tx.send(());
        }

        Ok(())
    }

    fn is_cancelled(&self) -> bool {
        match self.final_result.try_lock() {
            Ok(result) => matches!(&*result, Some(Err(AsyncTaskError::Cancelled))),
            Err(_) => false,
        }
    }

    fn on_cancel<F, Fut>(&self, _callback: F)
    where
        F: sweet_async_api::task::builder::AsyncWork<Fut> + Send + 'static,
        Fut: std::future::Future<Output = ()> + Send + 'static,
    {
        // TODO: Implement callback registration
    }

    fn cancel_gracefully(&self) -> impl Future<Output = Result<(), sweet_async_api::orchestra::OrchestratorError>> + Send {
        async move { self.cancel() }
    }

    fn cancel_forcefully(&self) -> impl Future<Output = Result<(), sweet_async_api::orchestra::OrchestratorError>> + Send {
        async move { self.cancel() }
    }

    fn cancel_immediately(&self) -> impl Future<Output = Result<(), sweet_async_api::orchestra::OrchestratorError>> + Send {
        async move { self.cancel() }
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::TracingTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn handle_error(
        &self,
        error: sweet_async_api::task::AsyncTaskError,
    ) -> Result<T, sweet_async_api::task::AsyncTaskError> {
        Err(error)
    }

    fn record_error(&self, error: &sweet_async_api::task::AsyncTaskError) {
        tracing::error!("Recording error: {:?}", error);
    }

    fn is_tracing_enabled(&self) -> bool {
        false
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::TimedTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn created_timestamp(&self) -> std::time::SystemTime {
        std::time::SystemTime::now()
    }

    fn executed_timestamp(&self) -> std::time::SystemTime {
        std::time::SystemTime::now()
    }

    fn completed_timestamp(&self) -> std::time::SystemTime {
        std::time::SystemTime::now()
    }

    fn timeout(&self) -> std::time::Duration {
        self.task_timeout
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::ContextualizedTask<T, I> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    type RuntimeType = crate::runtime::TokioRuntime;

    fn child_tasks(&self) -> Vec<T> {
        Vec::new()
    }

    fn parent(&self) -> Option<T> {
        None
    }

    fn runtime(&self) -> &Self::RuntimeType {
        panic!("ContextualizedTask::runtime is not directly available")
    }

    fn cwd(&self) -> std::path::PathBuf {
        std::env::current_dir().unwrap_or_else(|_| std::path::PathBuf::from("."))
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::StatusEnabledTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn status(&self) -> sweet_async_api::task::TaskStatus {
        if self.is_complete() {
            sweet_async_api::task::TaskStatus::Completed
        } else {
            sweet_async_api::task::TaskStatus::Running
        }
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::RecoverableTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    fn recover(
        &self,
        error: sweet_async_api::task::AsyncTaskError,
    ) -> Result<T, sweet_async_api::task::AsyncTaskError> {
        Err(error)
    }

    fn can_recover_from(&self, _error: &sweet_async_api::task::AsyncTaskError) -> bool {
        false
    }

    fn fallback_value(&self) -> Option<T> {
        None
    }
}

impl<T: Clone + Send + 'static, C: Clone + Send + 'static, EItem: Send + Sync + 'static, EOverall: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::MetricsEnabledTask<T> for TokioEmittingTask<T, C, EItem, EOverall, I>
{
    type Cpu = crate::task::async_task::TaskMetrics;
    type Memory = crate::task::async_task::TaskMetrics;
    type Io = crate::task::async_task::TaskMetrics;

    fn cpu_usage(&self) -> &Self::Cpu {
        &self.metrics
    }

    fn memory_usage(&self) -> &Self::Memory {
        &self.metrics
    }

    fn io_usage(&self) -> &Self::Io {
        &self.metrics
    }
}

impl<T: Clone + Send + Sync + 'static, C: Clone + Send + Sync + 'static, EItem: Send + Sync + 'static, I: TaskId>
    sweet_async_api::task::emit::builder::ReceiverBuilder<T, C, EItem, I> for TokioReceiverBuilder<T, C, EItem, I>
{
    type Task = TokioEmittingTask<T, C, EItem, AsyncTaskError, I>;

    fn run(self) -> Self::Task {
        let task_id_str = self.base_builder.get_name().unwrap_or_else(|| {
            format!(
                "emitting-task-{}",
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_nanos()
            )
        });
        let task_id = I::from_string(&task_id_str).unwrap_or_else(|| {
            let fallback_id_str = format!("fallback-emitting-task-{}", uuid::Uuid::new_v4());
            I::from_string(&fallback_id_str).expect("Failed to create TaskId for emitting task")
        });

        TokioEmittingTask::new(
            task_id,
            self.priority,
            self.sender_work,
            self.sender_strategy,
            self.receiver_work,
            self.receiver_strategy,
            self.runtime.clone(),
            self.base_builder.get_timeout(),
            self.active_tasks,
        )
    }

    async fn await_result(
        self,
    ) -> (C, <Self::Task as EmittingTask<T, C, EItem, I>>::Final) {
        let task = self.run();
        let final_event = task.await_final_event().await.unwrap();
        // Extract first successful result as the main result
        let first_result = final_event
            .collected_items
            .values()
            .find_map(|r| r.as_ref().ok().cloned())
            .unwrap();
        (first_result, final_event)
    }
}